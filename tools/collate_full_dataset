#! /usr/bin/env python3

import logging
logging.basicConfig(level=logging.INFO)

import argparse
import sys
import os

import numpy
import transformations

import csv
import json
import yaml
import pandas

from timeline import Timeline, TASKENGAGEMENT, SOCIALENGAGEMENT, SOCIALATTITUDE, MISSINGDATA

CHILDCHILD = "childchild"
CHILDROBOT = "childrobot"

# separator used to concatenate annotations when the do not match
SEPARATOR= "+"

MISSINGFRAME=-1

FPS=30.

PURPLE_TOPIC="camera_purple/rgb/image_raw/compressed"
YELLOW_TOPIC="camera_yellow/rgb/image_raw/compressed"

CONSTRUCTS = ["purple_child_task_engagement",
                "purple_child_social_engagement",
                "purple_child_social_attitude",
                "yellow_child_task_engagement",
                "yellow_child_social_engagement",
                "yellow_child_social_attitude"]


MIN_CONFIDENCE_FACE=0.2
MIN_CONFIDENCE_SKEL=0.05
MIN_CONFIDENCE_OPENFACE=0.7

NOSE_FACE=30 # idx of nose in the facial landmarks
NOSE_SKEL=0 # idx of nose in the skeleton joints

# face and skeleton indices are described here:
# https://github.com/freeplay-sandbox/analysis#format-of-poses-files
PURPLE_FACE_KEYS=["purple_child_face%02d_%s" % (i, s) for i in range(70) for s in 'xy']
YELLOW_FACE_KEYS=["yellow_child_face%02d_%s" % (i, s) for i in range(70) for s in 'xy']

PURPLE_SKEL_KEYS=["purple_child_skel%02d_%s" % (i, s) for i in range(18) for s in 'xy']
YELLOW_SKEL_KEYS=["yellow_child_skel%02d_%s" % (i, s) for i in range(18) for s in 'xy']

FIELDNAMES = ["timestamp",
              "id", 
              "condition", 
              "annotators", 
              "complete",
              "purple_child_age", "purple_child_gender", 
              "yellow_child_age", "yellow_child_gender"] + \
             ["purple_frame_idx"] + PURPLE_FACE_KEYS + PURPLE_SKEL_KEYS + \
             ["purple_child_head_x", "purple_child_head_y", "purple_child_head_z",
              "purple_child_head_rx", "purple_child_head_ry", "purple_child_head_rz", # head pose estimation, in mm and rad, relative to the table centre
              "purple_child_gaze_x", "purple_child_gaze_y", "purple_child_gaze_z", # gaze vector, averaged for both eyes, relative to the table centre.
              "purple_child_au01",
              "purple_child_au02",
              "purple_child_au04",
              "purple_child_au05",
              "purple_child_au06",
              "purple_child_au07",
              "purple_child_au09",
              "purple_child_au10",
              "purple_child_au12",
              "purple_child_au14",
              "purple_child_au15",
              "purple_child_au17",
              "purple_child_au20",
              "purple_child_au23",
              "purple_child_au25",
              "purple_child_au26",
              "purple_child_au28",
              "purple_child_au45"] +\
             ["purple_child_motion_intensity_avg",
              "purple_child_motion_intensity_stdev",
              "purple_child_motion_intensity_max",
              "purple_child_motion_direction_avg",
              "purple_child_motion_direction_stdev"] +\
             ["yellow_frame_idx"] + YELLOW_FACE_KEYS + YELLOW_SKEL_KEYS +\
             ["yellow_child_head_x", "yellow_child_head_y", "yellow_child_head_z",
              "yellow_child_head_rx", "yellow_child_head_ry", "yellow_child_head_rz", # head pose estimation, in mm and rad, relative to the table centre
              "yellow_child_gaze_x", "yellow_child_gaze_y", "yellow_child_gaze_z", # gaze vector, averaged for both eyes, relative to the table centre.
              "yellow_child_au01",
              "yellow_child_au02",
              "yellow_child_au04",
              "yellow_child_au05",
              "yellow_child_au06",
              "yellow_child_au07",
              "yellow_child_au09",
              "yellow_child_au10",
              "yellow_child_au12",
              "yellow_child_au14",
              "yellow_child_au15",
              "yellow_child_au17",
              "yellow_child_au20",
              "yellow_child_au23",
              "yellow_child_au25",
              "yellow_child_au26",
              "yellow_child_au28",
              "yellow_child_au45"] +\
              ["yellow_child_motion_intensity_avg",
               "yellow_child_motion_intensity_stdev",
               "yellow_child_motion_intensity_max",
               "yellow_child_motion_direction_avg",
               "yellow_child_motion_direction_stdev"] +\
             ["audio%02d" % i for i in range(16)] +\
             ["purple_child_task_engagement", "purple_child_social_engagement", "purple_child_social_attitude",
              "yellow_child_task_engagement", "yellow_child_social_engagement", "yellow_child_social_attitude"]

CHECKS_FIELDNAMES = ['id', 
                     'annotators',
                     'timestamp_bagfile_start', 
                     'timestamp_bagfile_end', 
                     'timestamp_first_purple_image', 
                     'timestamp_last_purple_image', 
                     'timestamp_first_yellow_image', 
                     'timestamp_last_yellow_image', 
                     'nb_purple_images_bag', 'nb_purple_poses', 
                     'nb_yellow_images_bag', 'nb_yellow_poses', 
                     'timestamp_first_annotation', 
                     'timestamp_last_annotation']

MAPPING_FACE = {
        "_child_head_x": " pose_Tx",
        "_child_head_y": " pose_Ty",
        "_child_head_z": " pose_Tz",
        "_child_head_rx": " pose_Rx",
        "_child_head_ry": " pose_Ry",
        "_child_head_rz": " pose_Rz"}

MAPPING_FACE.update({"_child_au%02d" % au: " AU%02d_r" % au for au in [1,2,4,5,6,7,9,10,12,14,15,17,20,23,25,26,45]})

MAPPING_GAZE = {
        "_child_gaze_angle_x": " gaze_angle_x",
        "_child_gaze_angle_y": " gaze_angle_y"}

##### Transformations of 3D points to the centre of the table

def make_transform_matrix(quaternion, translation):
    M = numpy.identity(4)
    T = transformations.translation_matrix(translation)
    M = numpy.dot(M, T)
    R = transformations.quaternion_matrix(quaternion)
    M = numpy.dot(M, R)

    M /= M[3, 3]

    return M
 
# Obtained with the following steps:
# $ rosparam set /use_sim_time True
# $ rosbag play --clock freeplay.bag
# $ rosrun tf static_transform_publisher -0.3 0.169 0 0 0 0 sandtray_centre sandtray 20
# $ rosrun tf tf_echo sandtray_centre camera_{purple|yellow}_rgb_optical_frame
YELLOW_CAM_TO_CENTRE_QUATERNION=[-0.530, 0.220, -0.314, 0.757]
YELLOW_CAM_TO_CENTRE_TRANSLATION=[-0.408, -0.208, 0.035]

PURPLE_CAM_TO_CENTRE_QUATERNION=[0.220, -0.530, 0.757, -0.314]
PURPLE_CAM_TO_CENTRE_TRANSLATION=[-0.408, 0.190, 0.035]

# numpy.dot({PURPLE,YELLOW}_CAM_TO_CENTRE, <vector>) transforms
# a vector <vector> from one of the camera's frame to the centre of the sandtray table frame.
YELLOW_CAM_TO_CENTRE =  make_transform_matrix(YELLOW_CAM_TO_CENTRE_QUATERNION, YELLOW_CAM_TO_CENTRE_TRANSLATION)
PURPLE_CAM_TO_CENTRE =  make_transform_matrix(PURPLE_CAM_TO_CENTRE_QUATERNION, PURPLE_CAM_TO_CENTRE_TRANSLATION)

########################################################

class Dataset:

    def __init__(self, root):

        self.dataset = []

        self.total_nb_recordings = 0
        self.paths = []

        logging.info("Looking for recordings...")

        for dirpath, dirs, files in os.walk(root, topdown=False):
            for name in files:
                fullpath = os.path.join(dirpath, name)
                if name == "experiment.yaml":
                    if dirpath.split(os.sep)[-1].startswith("exclude_"):
                        logging.debug("%s has been excluded" % fullpath)
                        continue
                    self.total_nb_recordings += 1
                    self.paths.append(dirpath)

        logging.info("Found %d valid recordings" % self.total_nb_recordings)
        logging.info("%d fields to store per recorded frame" % len(FIELDNAMES))

        self.last_timestamp = 0
        self.last_purple_timestamp = 0
        self.last_yellow_timestamp = 0

    def ischildchild(self, path):

        if not self.condition:
            with open(os.path.join(path, "experiment.yaml"), 'r') as yml:
                expe = yaml.load(yml)
                self.condition = expe["condition"]
        
        return self.condition == CHILDCHILD

    def frame_at(self, timestamp, frames, start_at=0):
        
        startidx=max(0, start_at)

        for idx, frame in enumerate(frames[startidx:]):
            if abs(frame["ts"] - timestamp) < 0.5 * 1/FPS:
                return startidx+idx, frame
            if frame["ts"] > timestamp + 1:
                #logging.error("Frame for timestamp %s not found!" % timestamp)
                return MISSINGFRAME, None

        #logging.error("Frame for timestamp %s not found!" % timestamp)
        return MISSINGFRAME, None

    def get_annotations(self, annotators_list, timestamp):

        annotators = annotators_list.split(SEPARATOR)

        result = {}

        for c in CONSTRUCTS:

            annotations = set()
            for annotator in annotators:
                ann = self.annotations[annotator][c].attime(timestamp)
                if ann != MISSINGDATA:
                    annotations.add(ann)

            if annotations:
                result[c] = SEPARATOR.join(annotations)

        return result

    def transform_gaze(self, side, raw_gaze_data):
        gaze0 = [float(raw_gaze_data[" gaze_0_x"]),
                 float(raw_gaze_data[" gaze_0_y"]),
                 float(raw_gaze_data[" gaze_0_z"]),
                 1]

        gaze1 = [float(raw_gaze_data[" gaze_1_x"]),
                 float(raw_gaze_data[" gaze_1_y"]),
                 float(raw_gaze_data[" gaze_1_z"]),
                 1]

        # average the gaze vectors of the 2 eyes + normalise
        gaze = transformations.unit_vector((numpy.array(gaze0) + numpy.array(gaze1))/2)

        # transform the result in the interactive table's frame
        return numpy.dot(PURPLE_CAM_TO_CENTRE if side == "purple" else YELLOW_CAM_TO_CENTRE, gaze).round(3)

    def transform_head_pose(self, side, raw_head_data):

        translation = [float(raw_head_data[" pose_Tx"]),
                       float(raw_head_data[" pose_Ty"]),
                       float(raw_head_data[" pose_Tz"])]
        euler = [float(raw_head_data[" pose_Rx"]),
                 float(raw_head_data[" pose_Ry"]),
                 float(raw_head_data[" pose_Rz"])]

        headpose = transformations.compose_matrix(angles=euler, translate=translation)

        transformed = numpy.dot(PURPLE_CAM_TO_CENTRE if side == "purple" else YELLOW_CAM_TO_CENTRE, headpose)

        _, _, euler_transformed, translation_transformed, _ = transformations.decompose_matrix(transformed)

        return numpy.array(translation_transformed).round(0), numpy.array(euler_transformed).round(3)




    def face_data_at(self, purple_frame_idx, yellow_frame_idx):

        result = {}

        # Note that OpenFace indexes frames starting at 1 while we start at 0 -> hence the +1 below.

        if not self.purple_face_csv.empty \
           and purple_frame_idx != MISSINGFRAME \
           and float(self.purple_face_csv.iloc[purple_frame_idx][" confidence"]) > MIN_CONFIDENCE_OPENFACE:

            # head pose
            purple_head_t, purple_head_r = self.transform_head_pose("purple", self.purple_face_csv.iloc[purple_frame_idx])

            result["purple_child_head_x"] = purple_head_t[0]
            result["purple_child_head_y"] = purple_head_t[1]
            result["purple_child_head_z"] = purple_head_t[2]

            result["purple_child_head_rx"] = purple_head_r[0]
            result["purple_child_head_ry"] = purple_head_r[1]
            result["purple_child_head_rz"] = purple_head_r[2]

            # gaze vector
            purple_gaze = self.transform_gaze("purple", self.purple_face_csv.iloc[purple_frame_idx])

            result["purple_child_gaze_x" ] = purple_gaze[0]
            result["purple_child_gaze_y" ] = purple_gaze[1]
            result["purple_child_gaze_z" ] = purple_gaze[2]


        if self.condition == CHILDCHILD \
           and not self.yellow_face_csv.empty \
           and yellow_frame_idx != MISSINGFRAME \
           and float(self.yellow_face_csv.iloc[yellow_frame_idx][" confidence"]) > MIN_CONFIDENCE_OPENFACE:

            # head pose
            yellow_head_t, yellow_head_r = self.transform_head_pose("yellow", self.yellow_face_csv.iloc[yellow_frame_idx])

            result["yellow_child_head_x"] = yellow_head_t[0]
            result["yellow_child_head_y"] = yellow_head_t[1]
            result["yellow_child_head_z"] = yellow_head_t[2]

            result["yellow_child_head_rx"] = yellow_head_r[0]
            result["yellow_child_head_ry"] = yellow_head_r[1]
            result["yellow_child_head_rz"] = yellow_head_r[2]

            # gaze vector
            yellow_gaze = self.transform_gaze("yellow", self.yellow_face_csv.iloc[yellow_frame_idx])
            result["yellow_child_gaze_x" ] = yellow_gaze[0]
            result["yellow_child_gaze_y" ] = yellow_gaze[1]
            result["yellow_child_gaze_z" ] = yellow_gaze[2]


        return result

    def optflow_data_at(self, purple_frame_idx, yellow_frame_idx):

        result = {}

        if self.purple_optflow_csv \
           and purple_frame_idx != MISSINGFRAME:

               row=self.purple_optflow_csv[purple_frame_idx]

               for k in row.keys():
                    if k != "frame":
                        result["purple_child_" + k.strip()] = row[k]

        if self.condition == CHILDCHILD \
           and self.yellow_optflow_csv \
           and yellow_frame_idx != MISSINGFRAME:

               row=self.yellow_optflow_csv[yellow_frame_idx]

               for k in row.keys():
                    if k != "frame":
                        result["yellow_child_" + k.strip()] = row[k]

        return result

            

    def processpath(self, path, writer, checks_writer, only_complete, duplicate_records):

        #################################################################
        #################################################################
        self.condition = None
        with open(os.path.join(path, "experiment.yaml"), 'r') as yml:
            expe = yaml.load(yml)

        #################################################################
        #################################################################
        with open(os.path.join(path, "freeplay.bag.yaml"), 'r') as yml:
            bagfile = yaml.load(yml)

        #################################################################
        #################################################################
        logging.info("Opening annotations...")

        self.annotations = {}
        annotator = None

        for file in os.listdir(path):
            if file.startswith("freeplay.annotations.") and file.endswith(".yaml"):
                annotationfile = os.path.join(path, file)
                annotator = file.split("freeplay.annotations.")[1].split(".yaml")[0]

                with open(annotationfile, 'r') as yml:
                    raw = yaml.load(yml)
                    annotation_timelines = {
                            "purple_child_task_engagement": Timeline(TASKENGAGEMENT, raw["purple"]),
                            "purple_child_social_engagement": Timeline(SOCIALENGAGEMENT, raw["purple"]),
                            "purple_child_social_attitude": Timeline(SOCIALATTITUDE, raw["purple"]),
                            "yellow_child_task_engagement": Timeline(TASKENGAGEMENT, raw["yellow"]),
                            "yellow_child_social_engagement": Timeline(SOCIALENGAGEMENT, raw["yellow"]),
                            "yellow_child_social_attitude": Timeline(SOCIALATTITUDE, raw["yellow"])
                            }
                    self.annotations[annotator] = annotation_timelines

        if len(self.annotations) == 0 and only_complete:
            logging.info("Skipping %s as no annotations and 'only complete' requested" % path)
            return
        if len(self.annotations) > 1:
            logging.info("%s: coded by %d annotators." % (path, len(self.annotations)))
            if duplicate_records:
                logging.info("--duplicate-records passed: I will accordingly duplicate this recording %d times." % len(self.annotations))
            else:
                logging.info("Divergent annotations will be concatenated.")

        #################################################################
        #################################################################
        logging.info("Opening freeplay.poses.json...")
        poses_json_file = os.path.join(path, "freeplay.poses.json")
        if not os.path.isfile(poses_json_file):
            logging.warning("%s has no freeplay.poses.json" % path)
            return

        with open(poses_json_file, 'r') as poses_json:
            poses = json.load(poses_json)

        #################################################################
        #################################################################
        ##### Loading face data

        self.purple_face_csv = {}
        self.yellow_face_csv = {}

        purple_face_csv_path = os.path.join(path, "videos", "processed", "camera_purple_raw.csv")
        yellow_face_csv_path = os.path.join(path, "videos", "processed", "camera_yellow_raw.csv")

        # purple face data should always exist
        if os.path.exists(purple_face_csv_path):
            logging.info("Reading face data %s..." % purple_face_csv_path)
            with open(purple_face_csv_path, 'r') as csv_file:
                self.purple_face_csv = pandas.read_csv(csv_file)

            if os.path.exists(yellow_face_csv_path):
                logging.info("Reading face data %s..." % yellow_face_csv_path)
                with open(yellow_face_csv_path, 'r') as csv_file:
                    self.yellow_face_csv = pandas.read_csv(csv_file)
        else:
            logging.error("Face data missing for %s" % path)


        #################################################################
        #################################################################
        ##### Loading optical flow data

        self.purple_optflow_csv = {}
        self.yellow_optflow_csv = {}

        purple_optflow_csv_path = os.path.join(path, "videos", "camera_purple_optical_flow.csv")
        yellow_optflow_csv_path = os.path.join(path, "videos", "camera_yellow_optical_flow.csv")
        if os.path.exists(purple_optflow_csv_path):
        
            logging.info("Reading optflow data %s..." % purple_optflow_csv_path)
            with open(purple_optflow_csv_path, 'r') as csv_file:
                reader = csv.DictReader(csv_file)
                for row in reader:
                    self.purple_optflow_csv[int(row["frame"])] = row

            # in some rare child-robot occasions, yellow optflow is missing
            if os.path.exists(yellow_optflow_csv_path):
                logging.info("Reading optflow data %s..." % yellow_optflow_csv_path)
                with open(yellow_optflow_csv_path, 'r') as csv_file:
                    reader = csv.DictReader(csv_file)
                    for row in reader:
                        self.yellow_optflow_csv[int(row["frame"])] = row
        else:
            logging.error("Optical flow data missing for %s" % path)


        #################################################################
        #################################################################
        #### COLLATE

        if duplicate_records:
            if len(self.annotations) == 0:
                localwriter = writer
                if localwriter is None:
                    logging.info("Writing to %s" % os.path.join(path,"pinsoro-complete-no-annotations.csv"))
                    f = open(os.path.join(path,"pinsoro-complete-no-annotations.csv"), 'w')
                    localwriter = csv.DictWriter(f, fieldnames=FIELDNAMES)
                    localwriter.writeheader()

                self.collate(path, localwriter, checks_writer, only_complete, None, expe, bagfile, poses)
            else:
                    for annotator in self.annotations.keys():
                        localwriter = writer
                        if localwriter is None:
                            logging.info("Writing to %s" % os.path.join(path,"pinsoro-complete-%s.csv" % annotator))
                            f = open(os.path.join(path,"pinsoro-complete-%s.csv" % annotator), 'w')
                            localwriter = csv.DictWriter(f, fieldnames=FIELDNAMES)
                            localwriter.writeheader()


                        self.collate(path, localwriter, checks_writer, only_complete, annotator, expe, bagfile, poses)

        else: # concatenate divergent annotations

            localwriter = writer
            if localwriter is None:

                id=path.split("/")[-1],
                logging.info("Writing to %s" % os.path.join(path,"pinsoro-%s.csv" % id))
                f = open(os.path.join(path,"pinsoro-%s.csv" % id), 'w')
                localwriter = csv.DictWriter(f, fieldnames=FIELDNAMES)
                localwriter.writeheader()

            annotators = SEPARATOR.join(self.annotations.keys())
            self.collate(path, localwriter, checks_writer, only_complete, annotators, expe, bagfile, poses)



    def collate(self, path, writer, checks_writer, only_complete, annotators, expe, bagfile, poses):
        #################################################################
        #################################################################
        #################################################################
        rows = []
        checks_row = {}

        ts_first_purple = poses[PURPLE_TOPIC]["frames"][0]["ts"]
        ts_first_yellow = poses[YELLOW_TOPIC]["frames"][0]["ts"]
        ts_last_purple = poses[PURPLE_TOPIC]["frames"][-1]["ts"]
        ts_last_yellow = poses[YELLOW_TOPIC]["frames"][-1]["ts"]

        ##############################################################
        ##############################################################
        #### Save dataset metadata for later sanity checks
        ##############################################################


        checks_row["id"] = path.split("/")[-1]
        if annotators:
            checks_row["annotators"] = annotators
        checks_row["timestamp_first_purple_image"] = ts_first_purple
        checks_row["timestamp_first_yellow_image"] = ts_first_yellow
        checks_row["timestamp_last_purple_image"] = ts_last_purple
        checks_row["timestamp_last_yellow_image"] = ts_last_yellow

        checks_row["timestamp_bagfile_start"] = bagfile["start"]
        checks_row["timestamp_bagfile_end"] = bagfile["end"]

        checks_row["nb_purple_images_bag"] = [t["messages"] for t in bagfile["topics"] if t["topic"] == PURPLE_TOPIC][0]
        checks_row["nb_purple_poses"] = len(poses[PURPLE_TOPIC]["frames"])
        checks_row["nb_yellow_images_bag"] = [t["messages"] for t in bagfile["topics"] if t["topic"] == YELLOW_TOPIC][0]
        checks_row["nb_yellow_poses"] = len(poses[YELLOW_TOPIC]["frames"])

        if annotators:
            checks_row["timestamp_first_annotation"] = "/".join(["%s: %s" % (a, min([t.start for t in self.annotations[a].values()])) for a in annotators.split(SEPARATOR)])
            checks_row["timestamp_last_annotation"] = "/".join(["%s: %s" % (a,max([t.end for t in self.annotations[a].values()])) for a in annotators.split(SEPARATOR)])

        if checks_writer:
            checks_writer.writerow(checks_row)

        ##############################################################
        ##############################################################
        ##############################################################


        # the starting timestamp is *not* bagfile["start"], but instead the timestamp contained in the header of the first published frame (which is *before* is was recorded in the bagfile.
        ts = min(ts_first_purple, ts_first_yellow)

        # we need to store the bagfile delta as annotations are timestamped relative to the bagfile start time, *not* the frames timestamps
        bagfile_delta = bagfile["start"] - ts

        last_ts = max(ts_last_purple, ts_last_yellow)

        duration = last_ts - ts
        logging.info("Collating features (%0.2f sec of data)..." % duration)

        last_percent = 0
        purple_idx = MISSINGFRAME
        last_good_purple_idx = MISSINGFRAME
        yellow_idx = MISSINGFRAME
        last_good_yellow_idx = MISSINGFRAME
        while ts < last_ts:

            skip_row = False

            row = {"timestamp": ts,
                    "condition": "childchild" if self.ischildchild(path) else "childrobot",
                    "id": path.split("/")[-1],
                    "purple_child_age": expe["purple-participant"]["age"],
                    "purple_child_gender": expe["purple-participant"]["gender"]}
            if self.ischildchild(path):
                row["yellow_child_age"] = expe["yellow-participant"]["age"]
                row["yellow_child_gender"] = expe["yellow-participant"]["gender"]
            else:
                row["yellow_child_gender"] = "robot"

            if annotators:
                row["annotators"] = annotators

            purple_idx, purple_frame = self.frame_at(ts, poses[PURPLE_TOPIC]["frames"], start_at=last_good_purple_idx)

            row["purple_frame_idx"] = purple_idx
            if purple_idx != MISSINGFRAME:
                last_good_purple_idx = purple_idx

            if purple_frame:
                row.update(
                    self.formatpose("faces", 
                                    purple_frame, 
                                    PURPLE_FACE_KEYS, 
                                    NOSE_FACE,
                                    MIN_CONFIDENCE_FACE))

                row.update(
                    self.formatpose("poses", 
                                    purple_frame, 
                                    PURPLE_SKEL_KEYS, 
                                    NOSE_SKEL,
                                    MIN_CONFIDENCE_SKEL))

            if self.condition == CHILDCHILD:

                yellow_idx, yellow_frame = self.frame_at(ts, poses[YELLOW_TOPIC]["frames"], start_at=last_good_yellow_idx)
                row["yellow_frame_idx"] = yellow_idx

                if yellow_idx != MISSINGFRAME:
                    last_good_yellow_idx = yellow_idx

                if yellow_frame:
                    row.update(
                        self.formatpose("faces", 
                                        yellow_frame, 
                                        YELLOW_FACE_KEYS, 
                                        NOSE_FACE,
                                        MIN_CONFIDENCE_FACE))

                    row.update(
                        self.formatpose("poses", 
                                        yellow_frame, 
                                        YELLOW_SKEL_KEYS, 
                                        NOSE_SKEL,
                                        MIN_CONFIDENCE_SKEL))


            face_data = self.face_data_at(purple_idx, yellow_idx)
            row.update(face_data)

            optflow_data = self.optflow_data_at(purple_idx, yellow_idx)
            row.update(optflow_data)


            if annotators:
                anns = self.get_annotations(annotators, ts + bagfile_delta)
                row.update(anns)

            row["complete"] = 1 if self.is_complete(row) else 0

            if row["complete"] or not only_complete:
                rows.append(row)

            ts += 1/FPS

            percent = int((ts - (last_ts-duration)) / duration * 100)
            if percent != last_percent:
                logging.info("%d%% done" % percent)
                last_percent = percent
        
        writer.writerows(rows)

    def is_complete(self, row):
        if row["purple_frame_idx"] == MISSINGFRAME \
           or "purple_child_head_x" not in row:
            return False

        if row["condition"] == "childchild" \
           and (row["yellow_frame_idx"] == MISSINGFRAME
                or "yellow_child_head_x" not in row):
            return False

        for c in CONSTRUCTS:
            if row[c] == "":
                return False

        return True


    def formatpose(self,
                    partname, 
                    frame, 
                    keys, 
                    idx_central_landmark, 
                    min_confidence_level):
        """
        :param idx_central_landmark: the index of one feature (for instance, the nose) that is expected to be close to the image centre. Used to select the right part when more than one face/skeleton has been detected.
        """

        ### First, select the face the closest to the center of the image

        if partname in frame and len(frame[partname]) > 0: # one face or more detected. Assume the correct one is the one closest to the image centre
            part = frame[partname]["1"]
            min_dist = pow(part[idx_central_landmark][0] - 0.5, 2) +\
                       pow(part[idx_central_landmark][1] - 0.5, 2)

            for p in frame[partname].values():
                dist = pow(p[idx_central_landmark][0] - 0.5, 2) +\
                       pow(p[idx_central_landmark][1] - 0.5, 2)

                if dist < min_dist:
                    part = p
                    min_dist = dist
        else:
            return {}

        results = {}
        for i in range(int(len(keys)/2)):
            x,y,c = part[i]
            if c > min_confidence_level:
                results[keys[i*2]] = x
                results[keys[i*2 + 1]] = y


        return results

    def write(self, writer, checks_writer, only_complete, duplicate_records):
        
        idx = 0
        for path in self.paths:
            idx += 1
            logging.info("[%02d/%02d] Processing %s" % (idx, len(self.paths), path))
            self.processpath(path, writer, checks_writer, only_complete, duplicate_records)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='PInSoRo Dataset -- Full post-processed dataset collator')
    parser.add_argument("--data-check", nargs='?', type=argparse.FileType('w'), default=False, const=sys.stdout, help="run some consistency checks on the dataset while collating it. You can specify a filename to save as CSV (default to stdout)")
    parser.add_argument("--only-complete", action="store_true", help="Only complete datapoints (all features and annotations present) are recorded")
    parser.add_argument("--single-csv", help="name of the CSV file where the whole dataset should be recorded. By default, create one file per recording.")
    parser.add_argument("--duplicate-records", help="some recordings have multiple annotators. By default, when annotations diverge, they are concatenated with '%s' as separator. If this option is passed, the whole recording is instead duplicated, one per annotator. This is especially useful when the data is used as training dataset." % SEPARATOR)
    parser.add_argument("path", help="root path of the dataset -- recordings are recursively looked for from this path")

    args = parser.parse_args()

    dataset = Dataset(args.path)

    writer=None
    if args.single_csv:
        f = open(args.file, 'w')
        writer = csv.DictWriter(f, fieldnames=FIELDNAMES)
        writer.writeheader()

    dataset_checks_writer = None
    if args.data_check:
        dataset_checks_writer = csv.DictWriter(args.data_check, fieldnames=CHECKS_FIELDNAMES)
        dataset_checks_writer.writeheader()

    dataset.write(writer, dataset_checks_writer, args.only_complete, args.duplicate_records)



